\documentclass[../main.tex]{subfiles}

\begin{document}
In this chapter, the results of the thesis are summarised and possibilities for future improvements will be discussed. 
\section{Conclusions}
This thesis aims at providing a way of safely learning the control for a system. Applying reinforcement learning methods to control tasks is a promising approach to overcome the strong dependence of traditional model-based control methods on accurate models especially for non-linear systems. Model-free reinforcement learning techniques such as Q-learning have been extensively studied and proven their value in various applications. However, the problem of how to satisfy constraints during the learning process has not yet been addressed. Therefore, Hamilton-Jacobi-Isaacs (HJI) reachability analysis has been incorporated into the algorithm to ensure safety during the learning process. By modelling the unknown parts of the state-space model as an unknown additive disturbance with known conservative bounds, this method provides a way to calculate a safe set. Within this set the system can move freely as long as a safe control is applied at the edges. Combining reinforcement learning and HJI analysis provides a way to safely learn a control for a system with uncertain dynamics. Both are introduced in Chapter \ref{sec:TheoreticalBackground}. To update the initial conservative bound on the disturbance, we then iteratively estimate the disturbance on the basis of collected data points. To this end, we apply Gaussian Processes (GP) that we introduce in Section \ref{sec:GP}. GP not only provide an estimate for the disturbance but also a measure for how certain this estimate is. By updating the disturbance with a safety margin based on the certainty of the estimate, we can precisely determine how good our safety guarantee is. \par
To make all methods run smoothly, we decided to run the algorithm on two different time-scales. The safety loop runs with a very small sample time to ensure that every constraint violation will be detected immediately. The reinforcement learning algorithm requires a longer time step to work properly. This idea is further described in Chapter \ref{sec:Implementation}.\par

Compared to the approach presented in \cite{akametalu2014reachability}, the method has been extended to incorporate exploration. In Chapter \ref{sec:Results}, we extensively compared the approaches with exploration and without exploration. We find that both, policy learning and disturbance estimation, can be considerably improved by encouraging exploration. This holds especially for the edges of the safe set that will barely be visited without exploration. 

\section{Future Work}

In what follows, some points that could be interesting to improve in the future will be listed. \par

\paragraph{Integration of Learning and Safety Loop.} Firstly, the collaboration of the different methods is not satisfactory in some regards. The safety loop of the algorithm solely passes an estimate of the disturbance bounds to the learning loop and runs apart from that completely separate. It would be preferable to have a joint design of safety loop and learning loop, i.e. to pass some more information from the safety loop to the learning loop to warm-start the Reinforcement Learning algorithm. For instance, the policy could be initialized to the safe control from the safety algorithm or conclusions about transition probabilities could be drawn from the recorded samples. Furthermore, the fact that the algorithm runs on two different time scales could render the algorithm suboptimal. Finding a way to run both loops on the same time scale might increase the efficiency of the algorithm substantially. Yet, at this point, we do not know how this could be achieved.\par
\paragraph{Recursive Disturbance Estimation.} Secondly, the disturbance estimation could possibly be improved. The GP regression is implemented batchwise with batches of $1000$ samples each. This is due to the computational complexity of the method. The disturbance estimation might be improved considerably by employing a recursive method that takes all recorded samples into account. It is however not trivial how to implement GP regression in a recursive manner because the method relies on re-computing the covariances for each new input.\par
\paragraph{Formal Guarantees.} 

Finally, the most significant improvement could be made by performing formal guarantees for the whole algorithm. This is however a very difficult matter, because we combine different methods that operate in different frameworks. For instance, the safe set calculations assume a continuous system evolution such that the safety-preserving control acts directly when the system hits the border of the safe set. On the contrary, the reinforcement learning algorithm operates in discrete time so that a safe set violation will only be detected at the next sample instance. This problem could for instance be tackled by shrinking the safe set with regard to the sampling time. The stochastic nature of Gaussian process regression causes further problems, because we require certain bounds on our disturbance in order to guarantee safety with HJI reachability analysis. The estimated disturbance range that we input is however not guaranteed to always hold, though we can make the probability of a disturbance outside that range very small. Performing any formal guarantees for all of the employed methods is therefore presumably very difficult, but would make the algorithm a lot more powerful. \par


In summary, we remark that ideas from this thesis could serve as a good starting point for future research. Even though some adjustments, i.e. a closer integration of the different methods, should be made, the approach of safe learning for control applications is really interesting and could overcome some of the limitations of traditional control.

\end{document}
\documentclass[../main.tex]{subfiles}

\begin{document}
\section{Motivation}
Applying methods from the field of machine learning to control problems is a promising approach. Many traditional control strategies rely on a model of the system that should be controlled. As models usually reflect only parts of the reality, those approaches often suffer from poor model accuracy. Learning the control in an online manner by employing standard Reinforcement Learning methods such as Q-Learning overcomes the shortcomings of traditional model-based control techniques. The problem with applying Reinforcement Learning methods to control problems is however that those methods learn in a trial-and-error fashion and are therefore not designed to satisfy constraints during the learning process. Safety critical control applications are therefore not feasible in this framework. A workaround is to assume some knowledge about the system and worst-case disturbances to design a safety-preserving controller that acts when the learning algorithm would cause a constraint violation. The disturbance bounds can then be updated iteratively by applying statistical modelling tools to obtained data. 

\section{General Idea}
This thesis introduces a framework for safely learning a control strategy for a given system with an additive disturbance with known bounds. On the basis of the known disturbance bounds a safe set in which the system can learn safely is estimated with Hamilton-Jacobi-Isaacs reachability analysis. Within that set, the Reinforcement Learning algorithm can choose learning actions freely as long as the safety-preserving control is applied when the system hits the edges of the safe set. After some learning episodes the disturbance bounds can be updated based on real-world data. To this end, Gaussian Process regression is conducted on the collected disturbance samples. 

\section{Outline}
The remaining parts of the thesis are structured as follows:
\begin{itemize}
\item \textbf{Chapter \ref{sec:TheoreticalBackground}} provides the reader with the theoretical background that is necessary to understand the details of the implementation. A short introduction is given to Reinforcement Learning, Gaussian Processes and Hamilton-Jacobi-Isaacs reachability analysis.
\item \textbf{Chapter \ref{sec:RelatedWork}} describes some related areas of research and presents the approach in \cite{akametalu2014reachability} that this thesis is largely based on.
\item \textbf{Chapter \ref{sec:SolutionArchitecture}} introduces the general framework in which the learning takes place. This chapter aims at giving a rough overview of the employed method without going into much detail.
\item \textbf{Chapter \ref{sec:Implementation}} describes the implementation details of the approach. This chapter builds largely on the theoretical background and the solution framework provided in chapters \ref{sec:TheoreticalBackground} and \ref{sec:SolutionArchitecture}.
\item  \textbf{Chapter \ref{sec:Results}} presents some of the findings from the implemented approach and briefly compares different methods.
\item Finally, \textbf{Chapter \ref{sec:Conclusions}} concludes the thesis and discusses possible suggestions for future work.
\end{itemize}
\end{document}
\documentclass[../main.tex]{subfiles}

\begin{document}
\renewcommand{\arraystretch}{1.3}

\section{Experimental Setup}

In this chapter, we numerically examine the approach described in Chapter \ref{sec:SolutionArchitecture}. The inverted pendulum system has been modelled as a MDP with $x_1\in [-\frac{\pi}{2}, \frac{\pi}{2}] $ and $x_2 \in [-5\, \frac{\text{rad}}{s},5 \,\frac{\text{rad}}{s}] $. The state space has been discretised with $19$ steps in each dimension and the action space has been discretised with $5$ steps on the range $u \in [-25\,\text{N},25\,\text{N}] $. All MDP parameters can also be found in table \ref{tab:MDP}. 

\begin{table}[ht]
\centering
\begin{tabular}{M{2.5cm}M{2.5cm}M{2.5cm}M{2.5cm}}
\hline \hline
      & min & max & $\#$ steps \\ \hline
$x_1$ & -$\frac{\pi}{2}$ & $\frac{\pi}{2}$ & 19 \\ 
$x_2$ & $ -5\, \frac{\text{rad}}{s}$ & $5\, \frac{\text{rad}}{s}$ & 19 \\  
$u$   & $-25\,\text{N}$ & $25\,\text{N}$ & 5 \\ \hline \hline
\end{tabular} 
\caption{\label{tab:MDP}MDP parameters.}
\end{table}


The time step for the learning loop is chosen as $h_\text{learn} = 0.2\,\text{s}$. The design parameters for the learning algorithm can be looked up in table \ref{tab:Learning}. 


\begin{table}[ht]
\centering
\begin{tabular}{M{3.0cm}M{3cm}}
\hline \hline
steps per iteration & 10,000 \\  
$h_\text{learn}$ & $0.2\,\text{s}$ \\ 
$\gamma$& 0.9\\ 
$m_0$ & 5 \\ 
$\varepsilon_\text{target}$ & 0.01 \\ 
$\varepsilon_0$ & 0.0001 \\ \hline \hline
\end{tabular} 
\caption{\label{tab:Learning}Learning parameters.}
\end{table}

The true disturbance introduced to the system is chosen to be $d = 0.5\sin(5x_1)$ and initially bounded conservatively with $\overline{d}_0 = 10 \,\text{N}$ and $\underline{d}_0 = -10 \,\text{N}$. The safe region $\mathcal{S}_0$ is a rectangle centered around the origin with width $0.9\pi$ in $x_1$-direction and width $10\frac{\text{rad}}{s}$ in $x_2$-direction. In each iteration the safe set is calculated for $\tau = 40\,\text{s}$. The safe set parameters are collected in table \ref{tab:SafeSet}.



\begin{table}[ht]
\centering
\begin{tabular}{M{2.5cm}M{2.5cm}M{2.5cm}}
\hline \hline
      & min & max  \\ \hline
$x_{1,\text{safe}}$ & $-0.45\pi$ & $0.45\pi$ \\ 

$x_{2,\text{safe}}$ & $ -5\, \frac{\text{rad}}{s}$ & $5\, \frac{\text{rad}}{s}$\\ 
$d_0$ & $-10\,\text{N}$ & $10\,\text{N}$ \\ \hline \hline
$h_\text{safe}$ & \multicolumn{2}{c}{$0.005\,\text{s}$}  \\ 
$\tau$ & \multicolumn{2}{c}{$40\,\text{s}$}  \\ \hline \hline
\end{tabular} 
\caption{\label{tab:SafeSet}Safe Set parameters.}
\end{table}


In the following part, the results from four learning iterations with each $10,000$ steps will be presented. After each iteration, a new disturbance estimation and safe set calculation is done. As described in Section \ref{sec:Exploration}, it is crucial for the disturbance and policy estimation that all states within the safe set are visited sufficiently often. A method for structured exploration, Incremental Q-learning, has been introduced. In this chapter, the results from incorporating exploration will be compared to those obtained from the approach without structured exploration. The four iterations take around $80\,\text{s}$ to terminate. The exact times for each iteration can be found in \ref{tab:time}. The remainder of the elapsed time is due to the calculation of the MDP and result plotting.


\begin{table}[ht]
\centering
\begin{tabular}{M{2.5cm}M{2.5cm}M{2.5cm}M{2.5cm}M{2.5cm}}
\hline \hline
  & Iteration 1 & Iteration 2 & Iteration 3 & Iteration 4 \\ 
\hline 
HJI analysis & 9.02 & 5.71 & 5.64 & 5.45 \\ 

RL & 8.47 & 8.49 & 8.36 & 8.37 \\ 

GP regression & 4.20 & 3.68 & 3.57 & 3.52 \\ 
\hline \hline
Total & \multicolumn{4}{c}{82.23} \\ 
\hline \hline
\end{tabular} 
\caption{\label{tab:time}Execution times for each iteration and in total. All times are given in seconds.}
\end{table}

\section{Policy Learning}

To begin with, we look at the distribution of $1000$ random samples drawn from the recorded samples of all four iterations. Figure \ref{fig:samples_exploration} shows the sampling results of the algorithm with exploration compared to the one without exploration. It can clearly be seen that the samples are a lot more wide-spread if exploration is done. Without exploration, the learning algorithm always decides for the action which is the most promising in order to keep the system close to the origin. This causes concentration of samples around this point that might inhibit both the disturbance estimation and the policy learning at the edges of the safe set. 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=0.8\textwidth]{samples_exploration}
    \end{subfigure}
    
    \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=0.8\textwidth]{samples_unexploration}
    \end{subfigure}
        \caption{Random sampling with and without Incremental Q-learning. In the upper figure, the spread of the samples is clearly better, whereas in the lower figure most samples are concentrated around the origin.}  \label{fig:samples_exploration}
\end{figure}

It is therefore interesting to examine whether the Incremental Q-learning, as described in Section \ref{sec:Exploration}, yields a better estimation of the policy. The results of this comparison are depicted in Figure \ref{fig:policy_exploration}. The figure shows the estimated policy from each state, where the respective colors correspond to values of $u$. The quality of the estimated policy can be decided upon by comparing to the optimal policy which has been found with Policy Iteration (left figure). Clearly, the results are better for the approach which incorporates exploration (middle figure). This can especially seen at the edges of the safe set, where the estimated policy without exploration is very inaccurate (right figure).

\begin{figure}[H]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{policy_exploration}}
    \caption{Policy estimated with Incremental Q-learning, $\epsilon$-greedy exploration and without exploration. After the same number of learning iterations, the algorithm with Incremental Q-learning achieves considerably better results above all at the edges of the safe set.} \label{fig:policy_exploration}
\end{figure}

To quantify the findings from Figure \ref{fig:policy_exploration}, the absolute error between the estimated and the optimal policy is illustrated in Figure \ref{fig:PolicyError}. In addition to the algorithm without exploration and with Incremental Q-learning, the error with $\epsilon$-greedy exploration is shown. It becomes apparent that the Incremental Q-learning approach takes slightly longer to converge but ends up with a remarkably better result. One can further conclude that the deterministic approach to exploration, Incremental Q-learning, pays off in comparison to the randomised $\epsilon$-greedy exploration. 

\begin{figure}[H]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{PolicyError}}
    \caption{Policy error with Incremental Q-learning, $\epsilon$-greedy eploration and without exploration. The approach with Incremental Q-learning converges slightly slower but the final error without exploration is about four times higher than the error of the Incremental Q-learning algorithm.} \label{fig:PolicyError}
\end{figure}

However, in the long run, pure exploration might not be the most desirable strategy. After all states have been visited sufficiently often, one would wish to exploit the most promising strategy. In the present system, this means that after sufficient exploration one would wish to keep the pendulum stable around the origin. This can for example be realised by switching off the exploration after all states have been visited $100$ times. It is therefore interesting to investigate, if the exploration algorithm can guarantee that all states are visited a sufficient number of times. Figure \ref{fig:leastvisited} shows the number of visits of the 10 least visited states with and without exploration. 
\begin{figure}[H]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=\textwidth]{leastvisited}}
        \caption{Number of visits of the 10 least visited states with and without exploration respectively.}  \label{fig:leastvisited}
\end{figure}
Without exploration, several states remain unvisited for all $40,000$ steps. None of the 10 least visited state is visited more than ten times. With Incremental Q-learning, the system passes all states within the first $20,000$ steps. Except from one, all states count more than $40$ visits within the observed $40,000$ steps. This indicates that it might indeed be possible to switch off the Incremental Q-learning after a certain number of iterations. A suitable criterion could be identified by examining how often a state approximately needs to be visited to eliminate the policy error. Yet, the topic of policy convergence has not yet been well covered as current results mostly focus on the convergence results of state values. \par
\section{Disturbance Estimation and Safe Set Computations}
In this section, the influence of exploration on the disturbance estimation will be examined. Figure \ref{fig:GP_it4_doubleexplore} shows the result from the disturbance estimation with GP regression after each iteration with the above described setup. As only states within the safe set can be visited, no estimate of a policy outside that set can be done. Therefore, only policy values within the safe set are compared. The left and right half of the figure show the result without exploration and Incremental Q-learning respectively. For both algorithms, it can be seen that the disturbance estimation improves after each iteration. This is due to the fact that the system visits wider regions of the state space with an increasing number of steps. Therefore, the samples taken from the recorded data points are spread out wider as more steps are recorded. Furthermore, it becomes evident that the disturbance estimation indeed is better for the approach with exploration. In the case without exploration, the samples are more concentrated in the front half of the coordinate system so that the disturbance estimate is good in that region but almost constant in the rear half and outside the safe set. In the case with exploration, the sinusoidal shape of the disturbance is clearly visible from the third iteration onwards. The disturbance estimate is quite accurate for all states within the safe set. 

\begin{figure}[H]
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{GP_it4_doubleexplore}}
        \caption{GP regression in four iterations with each 10,000 steps.}  \label{fig:GP_it4_doubleexplore}
\end{figure}


Figure \ref{fig:SafeSet_it4_unexplore} shows the safe set computations for all four iterations. During the first iteration, the conservative initial bound is used for the safe set calculations and gives a relatively small initial safe set. By calculating a better disturbance estimate in the GP regression, the safe set grows already largely in the second iteration. 
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{SafeSet_it4_unexplore}
        \caption{Safe set computation in four iterations. The better disturbance estimate causes an increase in the safe states already after the first iteration.}  \label{fig:SafeSet_it4_unexplore}
\end{figure}

Interestingly, the safe set resulting from the disturbance estimate is the same without and with exploration. This could be attributed to the fact that the safe set is an over-approximation of the true safe set that not necessarily increases as the disturbance bounds decrease. This claim is supported by the fact that the safe set remains the same even if a disturbance of $d = 0\,\text{N}$ is assumed.\par
\section{Simulations}
Finally, simulation results from both approaches are portrayed in Figure \ref{fig:Simulation}. The depicted trajectories show the last $500,000$ samples of the test run with four iterations that have been recorded with $h_\text{safe} = 0.005\,\text{s}$. It becomes clear that the algorithm with exploration causes the system to explore the whole state space and repeatedly hits the borders of the safe set. Since the safety check is not performed continuously, the system leaves the safe set at some points, but can each time be brought back into the set. In the right half of the figure it is apparent that the approach without exploration manages to keep the system close to the origin. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Simulation}
        \caption{Simulation of the system with Incremental Q-learning versus simulation without exploration. The figure shows the last $500,000$ samples of the test run in state space.}\label{fig:Simulation}
\end{figure}

To illustrate the whole learning process over time, simulation results with and without exploration are shown in \ref{fig:Simulation_time}. With exploration, the controller keeps exploring the borders of the safe set. Without exploration, the controller keeps the system closer to the origin after a policy has been learned. During the last 1500 seconds, the system never hits the borders of the safe set.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Simulation_time}
        \caption{Simulation of the system with Incremental Q-learning versus simulation without exploration. The figure shows the whole learning process over time (in seconds).}\label{fig:Simulation_time}
\end{figure}


\end{document}

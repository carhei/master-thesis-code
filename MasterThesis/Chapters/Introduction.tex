\documentclass[../main.tex]{subfiles}

\begin{document}
\section{Motivation}
Applying methods from the field of machine learning to control problems is a promising approach. Many traditional control strategies rely on a model of the system that shall be controlled. As models usually reflect only parts of the reality, those approaches often suffer from poor model accuracy. Learning the control in an online manner by employing standard reinforcement learning methods such as Q-Learning overcomes the shortcomings of traditional model-based control techniques. The problem with applying reinforcement learning methods to control tasks is that the challenge of satisfying constraints during the learning process is not well addressed yet. Safety-critical control applications are therefore not feasible in this framework. A workaround is to assume some knowledge about the system and worst-case disturbances to design a safety-preserving controller that acts when the learning algorithm would cause a constraint violation. The disturbance bounds can then be updated iteratively by applying statistical modelling tools to obtained data. \par
Applications of this framework could come from many different fields. One simple application is a lawn-mower robot that learns how to move within a garden without destroying the family's wading pool. Another one could be an unmanned aerial vehicle that should avoid hitting trees, houses, or humans. A third possible application of this framework could be an autonomous vehicle. Keeping these examples in mind, I will further illustrate our approach in the next section.
\section{General Idea}
This thesis implements a safe learning approach on an inverted pendulum. This system has the advantage to only have two states so that it can be analysed neatly and results can be illustrated fairly easily. As pendulums however not really are the ``safety-critical applications" you would come to think of first, I will explain the approach based on an autonomous vehicle. \par
Controlling such a vehicle with a standard model-based control technique, requires an accurate model of the state-space that might be difficult to obtain. To overcome this challenge, we could apply model-free reinforcement learning algorithms. This implies that the car itself would try a control action (e.g. steering left) and subsequently receive a reward that depends on if the car ended up in a desirable state (the middle of the road), or not (the roadside ditch). Based on this feedback, the vehicle could gradually learn a control policy that assigns control actions which make the car stay in the middle of the road to each state-action pair. \par
An obvious issue with this approach is that one might need to waste quite a lot of cars to roadside ditches before the controller finally learned an accurate policy. This becomes even more critical, if we regard a crowd of people alongside the road instead of a ditch. Therefore, we need to be able to ensure that the car never leaves the road. For example, one could apply a safety-preserving control each time the car gets too close to the edge of the road. An obvious question now is how to determine how close ``too close" is. This question is not trivial, because we do have uncertain dynamics. Therefore, we cannot predict exactly in which direction we turn after applying a certain control action. Luckily, there is way out of this dilemma that does not require a certain model of the system (which was something we wanted to avoid initially).\par 
Instead, we model the known parts of the vehicle into a state-space model and and think of the unknown parts of the model as an additive state-dependent disturbance. The only thing we then need is a conservative bound of this disturbance. Dependent on this bound and our known control limits, we can predict to which state the vehicle can get in the worst case, i.e. with a disturbance that drives the vehicle perpendicularly away from the middle of the street. Calculating all states that one should avoid because they potentially lead to an unsafe state is what Hamilton-Jacobi-Isaacs (HJI) reachability analysis does. Assuming a malicious disturbance and a control action that is trying to counteract that disturbance, HJI reachability analysis provides us with a safe set around the middle of the street and a safety-preserving control. If the vehicle applies this control at the edges of the safe set, no disturbance will ever manage to push it off the road. Inside the set, the learning controller can freely choose actions to learn a policy. This approach enables us to safely learn a control for the vehicle while staying on the road for all time. \par 
The only problem remaining is that the initial estimate for the disturbance bounds might be very conservative and lead to a small safe set. We therefore collect data samples while learning and estimate a less conservative disturbance model based on that. The method employed for disturbance estimation is Gaussian Process (GP) regression. GP regression is a valuable tool because it yields not only an estimate for the disturbance but also a measure for how certain this estimate is. We then can apply the new estimate with a safety margin that depends on the certainty of the estimate to calculate a larger safe set.

In summary, this thesis introduces a framework for safely learning a control strategy for a given system with an additive disturbance with known bounds. On the basis of the known disturbance range, a safe set in which the system can learn safely is estimated with HJI reachability analysis. Within that set, the Reinforcement Learning algorithm can choose learning actions freely as long as the safety-preserving control is applied when the system hits the edges of the safe set. After some learning episodes, the disturbance bounds can be updated based on real-world data. To this end, GP regression is conducted on the collected disturbance samples. 

\section{Outline}
The remaining parts of the thesis are structured as follows:
\begin{itemize}
\item \textbf{Chapter \ref{sec:TheoreticalBackground}} provides the reader with the theoretical background that is necessary to understand the details of the implementation. A short introduction is given to reinforcement learning, GP regression and HJI reachability analysis.
\item \textbf{Chapter \ref{sec:RelatedWork}} describes some related areas of research and presents the approach in \cite{akametalu2014reachability} that this thesis is largely based on.
\item \textbf{Chapter \ref{sec:SolutionArchitecture}} introduces the general framework in which the learning takes place. This chapter aims at giving a rough overview of the employed method without going into unnecessary detail.
\item \textbf{Chapter \ref{sec:Implementation}} describes the implementation details of the approach. This chapter builds largely on the theoretical background and the solution framework provided in chapters \ref{sec:TheoreticalBackground} and \ref{sec:SolutionArchitecture}.
\item  \textbf{Chapter \ref{sec:Results}} presents some of the findings from the implemented approach and briefly compares different methods.
\item Finally, \textbf{Chapter \ref{sec:Conclusions}} concludes the thesis and discusses possible suggestions for future work.
\end{itemize}
\end{document}
\documentclass[../main.tex]{subfiles}

\begin{document}
In this chapter, the results of the thesis and possibilities for future improvements will be discussed. 
\section{Conclusions}
This thesis aims at providing a way of safely learning the control for a system. Applying reinforcement learning methods to control tasks is a promising approach to overcome the strong dependence of traditional model-based control methods on accurate models. Model-free reinforcement learning techniques such as Q-learning have been extensively studied and proven their value in various applications. However, the problem of how to satisfy constraints during the learning process has not yet been addressed yet. Therefore, Hamilton-Jacobi-Isaacs (HJI) reachability analysis has been incorporated into the algorithm to ensure safety during the learning process. By modelling the unknown parts of the state-space model as an unknown additive disturbance with known conservative bounds, this method provides a way to calculate a safe set. Within this set the system can move freely as long as a safe control is applied at the edges. Combining reinforcement learning and HJI analysis provides a way to safely learn a control for a system with uncertain dynamics. To update the initial conservative bound on the disturbance, we then iteratively estimate the disturbance on the basis of collected data points. To this end, we apply Gaussian Processes that not only provide an estimate for the disturbance but also a measure for how certain this estimate is. Gaussian Process (GP) regression is a non-parametric regression method that is ideally suited for our purpose, since it is crucial to not under-approximate the disturbance. By updating the disturbance with a safety margin based on the certainty of the estimate, we can precisely say how good our safety guarantee is.\par
To make all methods run smoothly, we decided to run the algorithm on two different time-scales. The safety loop runs with a very small sample time to ensure that every constraint violation will be detected immediately. The reinforcement learning algorithm requires a longer time step to work properly.\par

Compared to the approach presented in \cite{akametalu2014reachability}, the method has been extended to incorporate exploration. In Chapter \ref{sec:Results}, we extensively compared the approaches with exploration and without exploration. We find that both, policy learning and disturbance estimation, can be considerably improved by encouraging exploration. This holds especially for the edges of the safe set that will barely be visited without exploration. 

\section{Future Work}

In the following section, some points that could be interesting to improve in the future will be listed. \par

\paragraph{Integration of Learning and Safety Loop.} Firstly, the collaboration of the different methods is not satisfactory in some regards. The safety loop of the algorithm solely passes an estimate of the disturbance bounds to the learning loop and runs apart from that completely separate. It would be preferable to have a joint design of safety loop and learning loop, i.e. to pass some more information from the safety loop to the learning loop to warm-start the Reinforcement Learning algorithm. For instance, the policy could be initialized to the safe control from the safety algorithm or conclusions about transition probabilities could be drawn from the recorded samples. Furthermore, the fact that the algorithm runs on two different time scales could render the algorithm suboptimal. Finding a way to run both loops on the same time scale might increase the efficiency of the algorithm substantially. Yet, we do not know at this point, how this could be achieved.\par
\paragraph{Recursive Disturbance Estimation.} Secondly, the disturbance estimation could possibly be improved. The GP regression is implemented batchwise with batches of $1000$ samples each. This is due to the computational complexity of the method. The disturbance estimation might be improved considerably by employing a recursive method that takes all recorded samples into account. It is however not trivial how to implement GP regression in a recursive manner because the method relies on re-computing the covariances for each new input.\par
\paragraph{Guarantee to always stay in the Safe Set.} Finally, the most significant improvement could be made by guaranteeing that the system never leaves the safe set. The two main problems that cause the system to sporadically leave the safe set in the current algorithm are known: Firstly, the numerical differentiation of the recorded data points leads to a faulty disturbance estimate, which distorts the safe set. Secondly, the safety check is not performed continuously which allows the system to leave the safe set in between two samples. The problem could for instance be tackled by shrinking the safe set with regard to the sampling time.\par
In summary, we remark that ideas from this thesis could serve as a good starting point for future research. Even though some adjustments, i.e. a closer integration of the different methods, should be made, the approach of safe learning for control applications is really interesting and could overcome some of the limitations of traditional control.

\end{document}
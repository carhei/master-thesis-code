\documentclass[../main.tex]{subfiles}

\begin{document}
The idea of improving the control of a system in an online manner is not a recent one. This section will describe some early approaches of ``learning control". There are two main strategies that researchers have been following. Firstly, learning methods can be employed to directly learn a control for a given system. Secondly, data can be used to improve the model of the system online.\par

An early example of the first approach is Iterative Learning Control \cite{aastrom2013adaptive}. In Iterative Learning Control, the controller incrementally improves its performance of a repetitive task. The control action is modified based on the control action of previous iterations and the deviation from a desired trajectory. However, this technique is constrained to a predefined repetitive reference trajectory with fixed durations of iterations and fixed initial conditions for each iteration.

A different example for an early contribution to this matter is the article of Wang et. al. \cite{wang1993stable} on stable adaptive fuzzy control. The proposed method assumes that the state space is of a special form
\begin{equation}
    x^{(n)} = f(x, \dot{x}, ..., x^{(n-1)})+bu, \qquad y=x
\end{equation}
with an unknown continuous function $f$ and an unknown constant $b$. A way is then presented on how to update the singleton parameters $\theta$ of a fuzzy controller
\begin{equation}
    u_c(x) = \theta^T \varepsilon(x)
\end{equation}
with fuzzy basis functions $\varepsilon(x)$. Assuming boundaries on $b$ and $f$, a supervisory controller that ensures stability is further introduced. The approach can only be applied to the above explained restricted class of state space models but is not limited to one trajectory. By incorporating the supervisory controller, it realizes some early form of ``safe learning for control".\par


The approach of learning the system model instead of a direct control has for instance been pursued in the field of Indirect Adaptive Control \cite{aastrom2013adaptive}. The adaptation of control parameters is here done by firstly estimating parameters of the model and subsequently computing control parameters based on that model. However, the estimation of parameters implicates that only a certain class of functions is considered. If the target function is not well modelled within that class, the control performance might still be poor. For this reason, Gaussian processes that are flexible enough to model a variety of different functions have been proven to be a valuable tool for non-linear modelling \cite{murphy2012machine}.\par

An approach that combines the indirect learning of system dynamics with the direct learning of control has been proposed in \cite{akametalu2014reachability}. This thesis is largely based on the method proposed in that article. The approach assumes an unknown, additive, state-dependent disturbance $d(x)$. The state space is assumed to be of the form:
\begin{equation}
    \dot{x} = h(x)+g(x)u+d(x),
\end{equation}
where $h(x)$ and $g(x)$ are the known parts of the system dynamics.
Assuming upper bounds on the disturbance, a safe set can be calculated using Hamilton-Jacobi-Isaacs (HJI) reachability analysis. Starting from a given conservative upper bound, the disturbance estimate is iteratively improved, leading to an increasing safe set. For states inside the safe set, a control is found with the learning algorithm Policy Gradient via the Signed Derivative (PGSD). Otherwise, a safe control that attempts to drive the system to the safest possible state is applied. In the objective function of the PGSD, a term is included that punishes switching between the safe and the learning control. This is done in order to reduce the number of times that the system reaches the border of the safe set. An issue with this approach is that exploration is not handled explicitly. This means that the algorithm does not encourage the acquisition of new knowledge about the state space. On the contrary, it discourages the system from taking exploratory actions towards the borders of the safe set by punishing switching between the safe and learning control. This conflicts with the goal of enlarging the safe set as the disturbance estimate will remain uncertain in regions where few samples have been gathered. \par

Because the approaches presented in \cite{akametalu2014reachability} and \cite{wang1993stable} both realise some form of safe learning, it might be interesting to compare the fundamentally different techniques they employ.  Compared to \cite{wang1993stable}, the method presented in \cite{akametalu2014reachability} poses less restrictions on the form of the state space. However, the way safety is ensured in this approach is a lot more complex than the computation of the supervisory control in \cite{wang1993stable}. The estimation of the disturbance and subsequent computation of a safe set is theoretically complex and computationally expensive. Both approaches have some critical assumptions: While the method in \cite{wang1993stable} relies on knowledge of the bounds of $f$ and $b$, that of \cite{akametalu2014reachability} assumes the knowledge of upper and lower bounds of $d$. Both assumptions might be hard to satisfy in practice. Finally, in the approach presented in \cite{akametalu2014reachability}, a control is learned via an learning algorithms that can be designed for performance. In \cite{wang1993stable}, the update of the parameter vector $\theta$ is fixed.

\end{document}
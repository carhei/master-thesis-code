\relax 
\providecommand*{\memsetcounter}[2]{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{1}}
\newlabel{sec:Introduction}{{\M@TitleReference {1}{Introduction}}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}General Idea}{1}}
\citation{akametalu2014reachability}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Outline}{3}}
\citation{sutton1998reinforcement}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Theoretical Background}{5}}
\newlabel{sec:TheoreticalBackground}{{\M@TitleReference {2}{Theoretical Background}}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The Reinforcement Learning Problem}{5}}
\newlabel{sec:RL}{{\M@TitleReference {2.1}{The Reinforcement Learning Problem}}{5}}
\citation{sutton1998reinforcement}
\citation{watkins1992q}
\citation{jaakkola1994convergence}
\@writefile{toc}{\contentsline {subsection}{Reinforcement learning algorithms}{7}}
\citation{azar2011speedy}
\citation{azar2011speedy}
\newlabel{eq:stepsize}{{\M@TitleReference {2.12}{Reinforcement learning algorithms}}{8}}
\newlabel{eq:optimistic_init}{{\M@TitleReference {2.14}{Reinforcement learning algorithms}}{8}}
\citation{strehl2006pac}
\citation{strehl2006pac}
\citation{tsitsiklis1997analysis}
\citation{murphy2012machine}
\citation{rasmussen2006gaussian}
\citation{rasmussen2006gaussian}
\newlabel{DelayedQ}{{\M@TitleReference {2.16}{Reinforcement learning algorithms}}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Gaussian Processes for Regression}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The upper figure shows samples from a Gaussian prior with a squared exponential kernel. The lower figure shows samples from the Gaussian posterior conditioned on four noisefree observations (yellow). Additionally, the mean function (black) and the $\pm 2 \sigma $-confidence region (grey) is shown.\relax }}{11}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:GP_example}{{\M@TitleReference {2.1}{The upper figure shows samples from a Gaussian prior with a squared exponential kernel. The lower figure shows samples from the Gaussian posterior conditioned on four noisefree observations (yellow). Additionally, the mean function (black) and the $\pm 2 \sigma $-confidence region (grey) is shown.\relax }}{11}}
\citation{rasmussen2006gaussian}
\citation{mitchell2004toolbox}
\citation{mitchell2003application}
\newlabel{eq:sqexp}{{\M@TitleReference {2.27}{Gaussian Processes for Regression}}{12}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Safe Set Computations based on Reachability Analysis}{12}}
\newlabel{sec:SafeSets}{{\M@TitleReference {2.3}{Safe Set Computations based on Reachability Analysis}}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The backwards reachable set $\mathcal  {T}_\tau $ is the set from which trajectories can reach the unsafe set within time $\tau $.\relax }}{13}}
\newlabel{fig:reachable}{{\M@TitleReference {2.2}{The backwards reachable set $\mathcal  {T}_\tau $ is the set from which trajectories can reach the unsafe set within time $\tau $.\relax }}{13}}
\newlabel{eq:hamil}{{\M@TitleReference {2.34}{Safe Set Computations based on Reachability Analysis}}{14}}
\citation{aastrom2013adaptive}
\citation{wang1993stable}
\citation{aastrom2013adaptive}
\citation{murphy2012machine}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Related Work}{15}}
\newlabel{sec:RelatedWork}{{\M@TitleReference {3}{Related Work}}{15}}
\citation{akametalu2014reachability}
\citation{akametalu2014reachability}
\citation{wang1993stable}
\citation{wang1993stable}
\citation{akametalu2014reachability}
\citation{wang1993stable}
\citation{wang1993stable}
\citation{akametalu2014reachability}
\citation{akametalu2014reachability}
\citation{wang1993stable}
\citation{akametalu2014reachability}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Solution Architecture}{17}}
\newlabel{sec:SolutionArchitecture}{{\M@TitleReference {4}{Solution Architecture}}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Rough Control Setup.\relax }}{18}}
\newlabel{fig:flow_simple}{{\M@TitleReference {4.1}{Rough Control Setup.\relax }}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Detailed Control Setup.\relax }}{19}}
\newlabel{fig:flow_complete}{{\M@TitleReference {4.2}{Detailed Control Setup.\relax }}{19}}
\citation{doya2000reinforcement}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Implementation}{21}}
\newlabel{sec:Implementation}{{\M@TitleReference {5}{Implementation}}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Markov Decision Process Model}{21}}
\newlabel{sec:implementation_MDP}{{\M@TitleReference {5.1}{Markov Decision Process Model}}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Reinforcement Learning}{22}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {Modification of Delayed Q-Learning}\relax }}{23}}
\newlabel{alg:alg1}{{\M@TitleReference {1}{\textsc  {Modification of Delayed Q-Learning}\relax }}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The learned policy versus the optimal policy.\relax }}{24}}
\newlabel{fig:PolicyComparison_unsafe}{{\M@TitleReference {5.1}{The learned policy versus the optimal policy.\relax }}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Evolution of the constraint violations with increasing number of steps.\relax }}{25}}
\newlabel{fig:Histogram_ConstraintViolations}{{\M@TitleReference {5.2}{Evolution of the constraint violations with increasing number of steps.\relax }}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Safe Set Computations based on Reachability Analysis}{25}}
\newlabel{eq:hamil_implementation}{{\M@TitleReference {5.7}{Safe Set Computations based on Reachability Analysis}}{25}}
\citation{mitchell2004toolbox}
\newlabel{eq:hamil_explicit}{{\M@TitleReference {5.14}{Safe Set Computations based on Reachability Analysis}}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Example for the evolution of a safe set during the time horizon.\relax }}{28}}
\newlabel{fig:safeSet_example}{{\M@TitleReference {5.3}{Example for the evolution of a safe set during the time horizon.\relax }}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Simulations to verify the control invariance of the safe set.\relax }}{29}}
\newlabel{fig:safeSet_simulation}{{\M@TitleReference {5.4}{Simulations to verify the control invariance of the safe set.\relax }}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Illustration of the problem with a large time step $h_{\text  {learn}}$. At $t= h_{\text  {learn}}$ the system is without safety check (left) already far outside the safe set so that it cannot be guaranteed to be brought back again. On the right hand side, the same example is shown with a faster running safety loop. Each $h_{\text  {safe}}$ a safety check is performed so that the violation of the safe set can be detected and acted against earlier.\relax }}{30}}
\newlabel{fig:doubleloop}{{\M@TitleReference {5.5}{Illustration of the problem with a large time step $h_{\text  {learn}}$. At $t= h_{\text  {learn}}$ the system is without safety check (left) already far outside the safe set so that it cannot be guaranteed to be brought back again. On the right hand side, the same example is shown with a faster running safety loop. Each $h_{\text  {safe}}$ a safety check is performed so that the violation of the safe set can be detected and acted against earlier.\relax }}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Disturbance Estimation with Gaussian processes}{30}}
\newlabel{sec:implementation_GP}{{\M@TitleReference {5.4}{Disturbance Estimation with Gaussian processes}}{30}}
\citation{Rasmussen:2010:GPM:1756006.1953029}
\citation{akametalu2014reachability}
\citation{even2001convergence}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Choice of samples (red) by employing a grid (blue) and taking the nearest neighbours to the grid points in order to ensure that the samples cover the whole state space.\relax }}{32}}
\newlabel{fig:pointchoice}{{\M@TitleReference {5.6}{Choice of samples (red) by employing a grid (blue) and taking the nearest neighbours to the grid points in order to ensure that the samples cover the whole state space.\relax }}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Exploration}{32}}
\newlabel{sec:Exploration}{{\M@TitleReference {5.5}{Exploration}}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Disturbance estimation with GP regression for a disturbance. The lower and upper planes bound the $\pm 3\sigma $ confidence interval (shaded in grey). The samples that serve as input to the GP are shown as blue circles. The mean plane can be seen as a thin line in the middle of the confidence interval. The lower figure depicts a slice through the middle of the figure to show the uncertainty in the middle of the surface.\relax }}{33}}
\newlabel{fig:GP_posterior}{{\M@TitleReference {5.7}{Disturbance estimation with GP regression for a disturbance. The lower and upper planes bound the $\pm 3\sigma $ confidence interval (shaded in grey). The samples that serve as input to the GP are shown as blue circles. The mean plane can be seen as a thin line in the middle of the confidence interval. The lower figure depicts a slice through the middle of the figure to show the uncertainty in the middle of the surface.\relax }}{33}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Experimental Results}{35}}
\newlabel{sec:Results}{{\M@TitleReference {6}{Experimental Results}}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Experimental Setup}{35}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces MDP parameters.\relax }}{35}}
\newlabel{tab:MDP}{{\M@TitleReference {6.1}{MDP parameters.\relax }}{35}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Learning parameters.\relax }}{35}}
\newlabel{tab:Learning}{{\M@TitleReference {6.2}{Learning parameters.\relax }}{35}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Safe Set parameters.\relax }}{36}}
\newlabel{tab:SafeSet}{{\M@TitleReference {6.3}{Safe Set parameters.\relax }}{36}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces Execution times for each iteration and in total. All times are given in seconds.\relax }}{36}}
\newlabel{tab:time}{{\M@TitleReference {6.4}{Execution times for each iteration and in total. All times are given in seconds.\relax }}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Policy Learning}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Random sampling with and without Incremental Q-learning. In the upper figure, the spread of the samples is clearly better, whereas in the lower figure most samples are concentrated around the origin.\relax }}{37}}
\newlabel{fig:samples_exploration}{{\M@TitleReference {6.1}{Random sampling with and without Incremental Q-learning. In the upper figure, the spread of the samples is clearly better, whereas in the lower figure most samples are concentrated around the origin.\relax }}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Policy estimated with Incremental Q-learning, $\epsilon $-greedy exploration and without exploration. After the same number of learning iterations, the algorithm with Incremental Q-learning achieves considerably better results above all at the edges of the safe set.\relax }}{38}}
\newlabel{fig:policy_exploration}{{\M@TitleReference {6.2}{Policy estimated with Incremental Q-learning, $\epsilon $-greedy exploration and without exploration. After the same number of learning iterations, the algorithm with Incremental Q-learning achieves considerably better results above all at the edges of the safe set.\relax }}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Policy error with Incremental Q-learning, $\epsilon $-greedy eploration and without exploration. The approach with Incremental Q-learning converges slightly slower but the final error without exploration is about four times higher than the error of the Incremental Q-learning algorithm.\relax }}{39}}
\newlabel{fig:PolicyError}{{\M@TitleReference {6.3}{Policy error with Incremental Q-learning, $\epsilon $-greedy eploration and without exploration. The approach with Incremental Q-learning converges slightly slower but the final error without exploration is about four times higher than the error of the Incremental Q-learning algorithm.\relax }}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Number of visits of the 10 least visited states with and without exploration respectively.\relax }}{40}}
\newlabel{fig:leastvisited}{{\M@TitleReference {6.4}{Number of visits of the 10 least visited states with and without exploration respectively.\relax }}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Disturbance Estimation and Safe Set Computations}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces GP regression in four iterations with each 10,000 steps.\relax }}{42}}
\newlabel{fig:GP_it4_doubleexplore}{{\M@TitleReference {6.5}{GP regression in four iterations with each 10,000 steps.\relax }}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Safe set computation in four iterations. The better disturbance estimate causes an increase in the safe states already after the first iteration.\relax }}{43}}
\newlabel{fig:SafeSet_it4_unexplore}{{\M@TitleReference {6.6}{Safe set computation in four iterations. The better disturbance estimate causes an increase in the safe states already after the first iteration.\relax }}{43}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Simulations}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Simulation of the system with Incremental Q-learning versus simulation without exploration.\relax }}{44}}
\newlabel{fig:Simulation}{{\M@TitleReference {6.7}{Simulation of the system with Incremental Q-learning versus simulation without exploration.\relax }}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Simulation of the system with Incremental Q-learning versus simulation without exploration.\relax }}{44}}
\newlabel{fig:Simulation}{{\M@TitleReference {6.8}{Simulation of the system with Incremental Q-learning versus simulation without exploration.\relax }}{44}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Conclusions and Future Work}{45}}
\newlabel{sec:Conclusions}{{\M@TitleReference {7}{Conclusions and Future Work}}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Conclusions}{45}}
\citation{akametalu2014reachability}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Future Work}{46}}
\@writefile{toc}{\contentsline {paragraph}{Integration of Learning and Safety Loop.}{46}}
\@writefile{toc}{\contentsline {paragraph}{Recursive Disturbance Estimation.}{46}}
\@writefile{toc}{\contentsline {paragraph}{Guarantee to always stay in the Safe Set.}{46}}
\bibdata{bibliography}
\bibcite{akametalu2014reachability}{1}
\bibcite{sutton1998reinforcement}{2}
\bibcite{watkins1992q}{3}
\bibcite{jaakkola1994convergence}{4}
\bibcite{azar2011speedy}{5}
\bibcite{strehl2006pac}{6}
\bibcite{tsitsiklis1997analysis}{7}
\bibcite{murphy2012machine}{8}
\bibcite{rasmussen2006gaussian}{9}
\bibcite{mitchell2004toolbox}{10}
\bibcite{mitchell2003application}{11}
\bibcite{aastrom2013adaptive}{12}
\bibcite{wang1993stable}{13}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{49}}
\bibcite{doya2000reinforcement}{14}
\bibcite{Rasmussen:2010:GPM:1756006.1953029}{15}
\bibcite{even2001convergence}{16}
\bibstyle{ieeetr}
\expandafter\gdef\csname eqp@this@COMMENT\endcsname{266.52696pt}
\expandafter\gdef\csname eqp@next@COMMENT\endcsname{0pt}
\memsetcounter{lastsheet}{54}
\memsetcounter{lastpage}{50}

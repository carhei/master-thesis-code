\relax 
\providecommand*{\memsetcounter}[2]{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{3}}
\newlabel{sec:Introduction}{{\M@TitleReference {1}{Introduction}}{3}}
\citation{sutton1998reinforcement}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Theoretical Background}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The Reinforcement Learning Problem}{5}}
\newlabel{sec:RL}{{\M@TitleReference {2.1}{The Reinforcement Learning Problem}}{5}}
\citation{watkins1992q}
\citation{jaakkola1994convergence}
\@writefile{toc}{\contentsline {subsection}{Reinforcement learning algorithms}{7}}
\newlabel{eq:stepsize}{{\M@TitleReference {2.9}{Reinforcement learning algorithms}}{7}}
\citation{azar2011speedy}
\citation{azar2011speedy}
\citation{strehl2006pac}
\citation{strehl2006pac}
\newlabel{eq:optimistic_init}{{\M@TitleReference {2.11}{Reinforcement learning algorithms}}{8}}
\newlabel{DelayedQ}{{\M@TitleReference {2.13}{Reinforcement learning algorithms}}{8}}
\citation{tsitsiklis1997analysis}
\citation{murphy2012machine}
\citation{rasmussen2006gaussian}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Gaussian Processes for Regression}{9}}
\citation{rasmussen2006gaussian}
\newlabel{eq:sqexp}{{\M@TitleReference {2.24}{Gaussian Processes for Regression}}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The upper figure shows samples from a Gaussian prior with a squared exponential kernel. The lower figure shows samples from the Gaussian posterior conditioned on four noisefree observations (yellow). Additionally, the mean function (black) and the $\pm 2 \sigma $-confidence region (grey) is shown.\relax }}{11}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:GP_example}{{\M@TitleReference {2.1}{The upper figure shows samples from a Gaussian prior with a squared exponential kernel. The lower figure shows samples from the Gaussian posterior conditioned on four noisefree observations (yellow). Additionally, the mean function (black) and the $\pm 2 \sigma $-confidence region (grey) is shown.\relax }}{11}}
\citation{mitchell2004toolbox}
\citation{mitchell2003application}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Safe Set Computations based on Reachability Analysis}{13}}
\newlabel{sec:SafeSets}{{\M@TitleReference {2.3}{Safe Set Computations based on Reachability Analysis}}{13}}
\newlabel{eq:hamil}{{\M@TitleReference {2.31}{Safe Set Computations based on Reachability Analysis}}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The backwards reachable set $\mathcal  {T}_\tau $ is the set from which trajectories can reach the unsafe set within time $\tau $.\relax }}{14}}
\newlabel{fig:reachable}{{\M@TitleReference {2.2}{The backwards reachable set $\mathcal  {T}_\tau $ is the set from which trajectories can reach the unsafe set within time $\tau $.\relax }}{14}}
\citation{aastrom2013adaptive}
\citation{wang1993stable}
\citation{aastrom2013adaptive}
\citation{murphy2012machine}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Related Work}{15}}
\citation{akametalu2014reachability}
\citation{akametalu2014reachability}
\citation{wang1993stable}
\citation{wang1993stable}
\citation{akametalu2014reachability}
\citation{wang1993stable}
\citation{wang1993stable}
\citation{akametalu2014reachability}
\citation{akametalu2014reachability}
\citation{wang1993stable}
\citation{akametalu2014reachability}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Solution Architecture}{17}}
\newlabel{sec:SolutionArchitecture}{{\M@TitleReference {4}{Solution Architecture}}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Rough Control Setup.\relax }}{18}}
\newlabel{fig:flow_simple}{{\M@TitleReference {4.1}{Rough Control Setup.\relax }}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Detailed Control Setup.\relax }}{19}}
\newlabel{fig:flow_complete}{{\M@TitleReference {4.2}{Detailed Control Setup.\relax }}{19}}
\citation{doya2000reinforcement}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Implementation}{21}}
\newlabel{sec:Implementation}{{\M@TitleReference {5}{Implementation}}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Markov Decision Process Model}{21}}
\newlabel{sec:implementation_MDP}{{\M@TitleReference {5.1}{Markov Decision Process Model}}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Reinforcement Learning}{22}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {Modification of Delayed Q-Learning}\relax }}{23}}
\newlabel{alg:alg1}{{\M@TitleReference {1}{\textsc  {Modification of Delayed Q-Learning}\relax }}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Comparison between the learned policy and the with policy iteration determined optimal policy.\relax }}{24}}
\newlabel{fig:PolicyComparison_unsafe}{{\M@TitleReference {5.1}{Comparison between the learned policy and the with policy iteration determined optimal policy.\relax }}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Evolution of the constraint violations with increasing number of steps.\relax }}{25}}
\newlabel{fig:Histogram_ConstraintViolations}{{\M@TitleReference {5.2}{Evolution of the constraint violations with increasing number of steps.\relax }}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Safe Set Computations based on Reachability Analysis}{25}}
\newlabel{eq:hamil_implementation}{{\M@TitleReference {5.7}{Safe Set Computations based on Reachability Analysis}}{25}}
\citation{mitchell2004toolbox}
\newlabel{eq:hamil_explicit}{{\M@TitleReference {5.18}{Safe Set Computations based on Reachability Analysis}}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Example for the evolution of a safe set during the time horizon.\relax }}{28}}
\newlabel{fig:safeSet_example}{{\M@TitleReference {5.3}{Example for the evolution of a safe set during the time horizon.\relax }}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Simulations to verify the control invariance of the safe set.\relax }}{29}}
\newlabel{fig:safeSet_simulation}{{\M@TitleReference {5.4}{Simulations to verify the control invariance of the safe set.\relax }}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Illustration of the problem with a large time step $h_{\text  {learn}}$. At $t= h_{\text  {learn}}$ the system is without safety check (left) already far outside the safe set so that it cannot be guaranteed to be brought back again. On the right hand side, the same example is shown with a faster running safety loop. Each $h_{\text  {safe}}$ a safety check is performed so that the violation of the safe set can be detected and acted against earlier.\relax }}{30}}
\newlabel{fig:doubleloop}{{\M@TitleReference {5.5}{Illustration of the problem with a large time step $h_{\text  {learn}}$. At $t= h_{\text  {learn}}$ the system is without safety check (left) already far outside the safe set so that it cannot be guaranteed to be brought back again. On the right hand side, the same example is shown with a faster running safety loop. Each $h_{\text  {safe}}$ a safety check is performed so that the violation of the safe set can be detected and acted against earlier.\relax }}{30}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Disturbance Estimation with Gaussian Processes}{30}}
\newlabel{sec:implementation_GP}{{\M@TitleReference {5.4}{Disturbance Estimation with Gaussian Processes}}{30}}
\citation{Rasmussen:2010:GPM:1756006.1953029}
\citation{akametalu2014reachability}
\citation{even2001convergence}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Choice of samples (red) by employing a grid (blue) and taking the nearest neighbours to the grid points in order to ensure that the samples cover the whole state space.\relax }}{32}}
\newlabel{fig:pointchoice}{{\M@TitleReference {5.6}{Choice of samples (red) by employing a grid (blue) and taking the nearest neighbours to the grid points in order to ensure that the samples cover the whole state space.\relax }}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Exploration}{32}}
\newlabel{sec:Exploration}{{\M@TitleReference {5.5}{Exploration}}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Disturbance estimation with GP regression for a disturbance. The lower and upper plane bound the $\pm 3\sigma $ confidence interval (shaded in grey). The samples that serve as input to the GP are shown as blue circles. The mean plane can be seen as a thin line in the middle of the confidence interval. The lower figure shows the rear half of the upper figure to show the uncertainty in the middle of the surface.\relax }}{33}}
\newlabel{fig:GP_posterior}{{\M@TitleReference {5.7}{Disturbance estimation with GP regression for a disturbance. The lower and upper plane bound the $\pm 3\sigma $ confidence interval (shaded in grey). The samples that serve as input to the GP are shown as blue circles. The mean plane can be seen as a thin line in the middle of the confidence interval. The lower figure shows the rear half of the upper figure to show the uncertainty in the middle of the surface.\relax }}{33}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Results}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Random sampling with and without Incremental Q-learning. In the upper figure, the spread of the samples is clearly better, whereas in the lower figure most samples are concentrated around the origin.\relax }}{36}}
\newlabel{fig:samples_exploration}{{\M@TitleReference {6.1}{Random sampling with and without Incremental Q-learning. In the upper figure, the spread of the samples is clearly better, whereas in the lower figure most samples are concentrated around the origin.\relax }}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Policy estimated with and without exploration. After the same number of learning iterations, the algorithm with exploration achieves considerably better results above all at the edges of the safe set.\relax }}{37}}
\newlabel{fig:policy_exploration}{{\M@TitleReference {6.2}{Policy estimated with and without exploration. After the same number of learning iterations, the algorithm with exploration achieves considerably better results above all at the edges of the safe set.\relax }}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Policy error with Incremental Q-learning, $\epsilon $-greedy eploration and without exploration. The approach with Incremental Q-learning converges slightly slower but the final error without exploration is about four times higher than the error of the Incremental Q-learning algorithm.\relax }}{38}}
\newlabel{fig:PolicyError}{{\M@TitleReference {6.3}{Policy error with Incremental Q-learning, $\epsilon $-greedy eploration and without exploration. The approach with Incremental Q-learning converges slightly slower but the final error without exploration is about four times higher than the error of the Incremental Q-learning algorithm.\relax }}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Number of visits of the ten least visited states with and without exploration respectively.\relax }}{39}}
\newlabel{fig:leastvisited}{{\M@TitleReference {6.4}{Number of visits of the ten least visited states with and without exploration respectively.\relax }}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Gaussian Process regression in four iterations.\relax }}{40}}
\newlabel{fig:GP_it4_doubleexplore}{{\M@TitleReference {6.5}{Gaussian Process regression in four iterations.\relax }}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Safe set computation in four iterations. The better disturbance estimate causes an increase in the safe states already after the first iteration.\relax }}{41}}
\newlabel{fig:SafeSet_it4_unexplore}{{\M@TitleReference {6.6}{Safe set computation in four iterations. The better disturbance estimate causes an increase in the safe states already after the first iteration.\relax }}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Choice of samples (red) by employing a grid (blue) and taking the nearest neighbours to the grid points in order to ensure that the samples cover the whole state space.\relax }}{42}}
\newlabel{fig:Simulation}{{\M@TitleReference {6.7}{Choice of samples (red) by employing a grid (blue) and taking the nearest neighbours to the grid points in order to ensure that the samples cover the whole state space.\relax }}{42}}
\bibdata{bibliography}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Conclusions}{43}}
\bibcite{akametalu2014reachability}{1}
\bibcite{aastrom2013adaptive}{2}
\bibcite{azar2011speedy}{3}
\bibcite{doya2000reinforcement}{4}
\bibcite{even2001convergence}{5}
\bibcite{jaakkola1994convergence}{6}
\bibcite{mitchell2003application}{7}
\bibcite{mitchell2004toolbox}{8}
\bibcite{murphy2012machine}{9}
\bibcite{rasmussen2006gaussian}{10}
\bibcite{Rasmussen:2010:GPM:1756006.1953029}{11}
\bibcite{strehl2006pac}{12}
\bibcite{sutton1998reinforcement}{13}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{45}}
\bibcite{tsitsiklis1997analysis}{14}
\bibcite{wang1993stable}{15}
\bibcite{watkins1992q}{16}
\bibstyle{abbrv}
\expandafter\gdef\csname eqp@this@COMMENT\endcsname{266.52696pt}
\expandafter\gdef\csname eqp@next@COMMENT\endcsname{0pt}
\memsetcounter{lastsheet}{46}
\memsetcounter{lastpage}{46}

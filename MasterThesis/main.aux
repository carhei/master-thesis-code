\relax 
\providecommand*{\memsetcounter}[2]{}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {1}Introduction}{3}}
\newlabel{sec:Introduction}{{\M@TitleReference {1}{Introduction}}{3}}
\citation{sutton1998reinforcement}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {2}Theoretical Background}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The Reinforcement Learning Problem}{5}}
\newlabel{sec:RL}{{\M@TitleReference {2.1}{The Reinforcement Learning Problem}}{5}}
\citation{watkins1992q}
\citation{jaakkola1994convergence}
\@writefile{toc}{\contentsline {subsection}{Reinforcement learning algorithms}{7}}
\newlabel{eq:stepsize}{{\M@TitleReference {2.9}{Reinforcement learning algorithms}}{7}}
\citation{azar2011speedy}
\citation{azar2011speedy}
\citation{strehl2006pac}
\citation{strehl2006pac}
\newlabel{eq:optimistic_init}{{\M@TitleReference {2.11}{Reinforcement learning algorithms}}{8}}
\newlabel{DelayedQ}{{\M@TitleReference {2.13}{Reinforcement learning algorithms}}{8}}
\citation{tsitsiklis1997analysis}
\citation{murphy2012machine}
\citation{rasmussen2006gaussian}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Gaussian Processes for Regression}{9}}
\citation{rasmussen2006gaussian}
\newlabel{eq:sqexp}{{\M@TitleReference {2.24}{Gaussian Processes for Regression}}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The upper figure shows samples from a Gaussian prior with a squared exponential kernel. The lower figure shows samples from the Gaussian posterior conditioned on four noisefree observations (yellow). Additionally, the mean function (black) and the $\pm 2 \sigma $-confidence region (grey) is shown.\relax }}{11}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:GP_example}{{\M@TitleReference {2.1}{The upper figure shows samples from a Gaussian prior with a squared exponential kernel. The lower figure shows samples from the Gaussian posterior conditioned on four noisefree observations (yellow). Additionally, the mean function (black) and the $\pm 2 \sigma $-confidence region (grey) is shown.\relax }}{11}}
\citation{mitchell2004toolbox}
\citation{mitchell2003application}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Safe Set Computations based on Reachability Analysis}{13}}
\newlabel{sec:SafeSets}{{\M@TitleReference {2.3}{Safe Set Computations based on Reachability Analysis}}{13}}
\newlabel{eq:hamil}{{\M@TitleReference {2.31}{Safe Set Computations based on Reachability Analysis}}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The backwards reachable set $\mathcal  {T}_\tau $ is the set from which trajectories can reach the unsafe set within time $\tau $.\relax }}{14}}
\newlabel{fig:reachable}{{\M@TitleReference {2.2}{The backwards reachable set $\mathcal  {T}_\tau $ is the set from which trajectories can reach the unsafe set within time $\tau $.\relax }}{14}}
\citation{murphy2012machine}
\citation{aastrom2013adaptive}
\citation{sutton1998reinforcement}
\citation{wang1993stable}
\citation{akametalu2014reachability}
\citation{wang1993stable}
\citation{akametalu2014reachability}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Related Work}{15}}
\citation{akametalu2014reachability}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {4}Solution Architecture}{17}}
\newlabel{sec:SolutionArchitecture}{{\M@TitleReference {4}{Solution Architecture}}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Rough Control Setup.\relax }}{18}}
\newlabel{fig:flow_simple}{{\M@TitleReference {4.1}{Rough Control Setup.\relax }}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Detailed Control Setup.\relax }}{19}}
\newlabel{fig:flow_complete}{{\M@TitleReference {4.2}{Detailed Control Setup.\relax }}{19}}
\citation{doya2000reinforcement}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {5}Implementation}{21}}
\newlabel{sec:Implementation}{{\M@TitleReference {5}{Implementation}}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Markov Decision Process Model}{21}}
\newlabel{sec:implementation_MDP}{{\M@TitleReference {5.1}{Markov Decision Process Model}}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Reinforcement Learning}{22}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces \textsc  {Modification of Delayed Q-Learning}\relax }}{23}}
\newlabel{alg:alg1}{{\M@TitleReference {1}{\textsc  {Modification of Delayed Q-Learning}\relax }}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Safe Set Computations based on Reachability Analysis}{23}}
\citation{mitchell2004toolbox}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Comparison between the learned policy and the with policy iteration determined optimal policy.\relax }}{24}}
\newlabel{fig:PolicyComparison_unsafe}{{\M@TitleReference {5.1}{Comparison between the learned policy and the with policy iteration determined optimal policy.\relax }}{24}}
\newlabel{eq:hamil_implementation}{{\M@TitleReference {5.7}{Safe Set Computations based on Reachability Analysis}}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Evolution of the constraint violations over .\relax }}{25}}
\newlabel{fig:Histogram_ConstraintViolations}{{\M@TitleReference {5.2}{Evolution of the constraint violations over .\relax }}{25}}
\newlabel{eq:hamil_explicit}{{\M@TitleReference {5.18}{Safe Set Computations based on Reachability Analysis}}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Example for the evolution of a safe set during the time horizon.\relax }}{27}}
\newlabel{fig:safeSet_example}{{\M@TitleReference {5.3}{Example for the evolution of a safe set during the time horizon.\relax }}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Simulations to verify that all trajectories that start.\relax }}{28}}
\newlabel{fig:safeSet_simulation}{{\M@TitleReference {5.4}{Simulations to verify that all trajectories that start.\relax }}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Illustration of the problem with a large time step $h_{\text  {learn}}$. At $t= h_{\text  {learn}}$ the system is without safety check (left) already far outside the safe set so that it cannot be guaranteed to be brought back again. On the right hand side, the same example is shown with a faster running safety loop. Each $h_{\text  {safe}}$ a safety check is performed so that the violation of the safe set can be detected and acted against earlier\relax }}{29}}
\newlabel{fig:doubleloop}{{\M@TitleReference {5.5}{Illustration of the problem with a large time step $h_{\text  {learn}}$. At $t= h_{\text  {learn}}$ the system is without safety check (left) already far outside the safe set so that it cannot be guaranteed to be brought back again. On the right hand side, the same example is shown with a faster running safety loop. Each $h_{\text  {safe}}$ a safety check is performed so that the violation of the safe set can be detected and acted against earlier\relax }}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Disturbance Estimation with Gaussian Processes}{29}}
\newlabel{sec:implementation_GP}{{\M@TitleReference {5.4}{Disturbance Estimation with Gaussian Processes}}{29}}
\citation{Rasmussen:2010:GPM:1756006.1953029}
\citation{akametalu2014reachability}
\citation{even2001convergence}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Choice of samples (red) by employing a grid (blue) and taking the nearest neighbours to the grid points in order to ensure that the samples cover the whole state space.\relax }}{31}}
\newlabel{fig:pointchoice}{{\M@TitleReference {5.6}{Choice of samples (red) by employing a grid (blue) and taking the nearest neighbours to the grid points in order to ensure that the samples cover the whole state space.\relax }}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Exploration}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Disturbance estimation with GP regression for a disturbance. The lower and upper plane bound the $\pm 3\sigma $ confidence interval (shaded in grey). The samples that serve as input to the GP are shown as blue circles. The mean plane can be seen as a thin line in the middle of the confidence interval.\relax }}{32}}
\newlabel{fig:GP_posterior}{{\M@TitleReference {5.7}{Disturbance estimation with GP regression for a disturbance. The lower and upper plane bound the $\pm 3\sigma $ confidence interval (shaded in grey). The samples that serve as input to the GP are shown as blue circles. The mean plane can be seen as a thin line in the middle of the confidence interval.\relax }}{32}}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {6}Results}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Safe Set Computation in four iterations. Clearly, the safe set already converges after the first iteration.\relax }}{35}}
\newlabel{fig:SafeSet_it4_unexplore}{{\M@TitleReference {6.1}{Safe Set Computation in four iterations. Clearly, the safe set already converges after the first iteration.\relax }}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Logarithmic Error of the learned values compared to the values computed by Value Iteration. Each iteration consists of $10000$ steps.\relax }}{36}}
\newlabel{fig:Learning_it4_unexplore}{{\M@TitleReference {6.2}{Logarithmic Error of the learned values compared to the values computed by Value Iteration. Each iteration consists of $10000$ steps.\relax }}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Gaussian Process regression in four iterations.\relax }}{36}}
\newlabel{fig:GP_it4_unexplore}{{\M@TitleReference {6.3}{Gaussian Process regression in four iterations.\relax }}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Gaussian Process regression in four iterations with incremental Q-learning.\relax }}{37}}
\newlabel{fig:GP_it4_explore}{{\M@TitleReference {6.4}{Gaussian Process regression in four iterations with incremental Q-learning.\relax }}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces Random sampling with and without incremental Q-learning. In the upper figure, the spread of the samples is clearly better, whereas in the lower figure most samples are concentrated around the origin.\relax }}{38}}
\newlabel{fig:samples_exploration}{{\M@TitleReference {6.5}{Random sampling with and without incremental Q-learning. In the upper figure, the spread of the samples is clearly better, whereas in the lower figure most samples are concentrated around the origin.\relax }}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Policy estimation with and without exploration. After the same number of learning iterations, the algorithm with exploration achieves considerably better results above all at the edges of the safe set.\relax }}{39}}
\newlabel{fig:policy_exploration}{{\M@TitleReference {6.6}{Policy estimation with and without exploration. After the same number of learning iterations, the algorithm with exploration achieves considerably better results above all at the edges of the safe set.\relax }}{39}}
\bibdata{bibliography}
\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\contentsline {chapter}{\chapternumberline {7}Conclusions}{41}}
\bibcite{akametalu2014reachability}{1}
\bibcite{aastrom2013adaptive}{2}
\bibcite{azar2011speedy}{3}
\bibcite{doya2000reinforcement}{4}
\bibcite{even2001convergence}{5}
\bibcite{jaakkola1994convergence}{6}
\bibcite{mitchell2003application}{7}
\bibcite{mitchell2004toolbox}{8}
\bibcite{murphy2012machine}{9}
\bibcite{rasmussen2006gaussian}{10}
\bibcite{Rasmussen:2010:GPM:1756006.1953029}{11}
\bibcite{strehl2006pac}{12}
\bibcite{sutton1998reinforcement}{13}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{43}}
\bibcite{tsitsiklis1997analysis}{14}
\bibcite{wang1993stable}{15}
\bibcite{watkins1992q}{16}
\bibstyle{abbrv}
\memsetcounter{lastsheet}{44}
\memsetcounter{lastpage}{44}
